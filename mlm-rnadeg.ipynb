{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":22111,"databundleVersionId":1600624,"sourceType":"competition"},{"sourceId":7011682,"sourceType":"datasetVersion","datasetId":4026855},{"sourceId":7526572,"sourceType":"datasetVersion","datasetId":4383896}],"dockerImageVersionId":30636,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install optuna","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\nimport torch\nprint(torch.__version__)\n!python --version\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T00:39:00.079413Z","iopub.execute_input":"2024-02-13T00:39:00.079668Z","iopub.status.idle":"2024-02-13T00:39:23.895329Z","shell.execute_reply.started":"2024-02-13T00:39:00.079643Z","shell.execute_reply":"2024-02-13T00:39:23.894199Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"TensorFlow Version: 2.13.0\nCUDA Version TensorFlow was built with: 11.8\ncuDNN Version TensorFlow was built with: 8\nPyTorch Version: 2.0.0\nCUDA Version PyTorch was built with: 11.8\ncuDNN Version: 8900\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Wed_Sep_21_10:33:58_PDT_2022\nCuda compilation tools, release 11.8, V11.8.89\nBuild cuda_11.8.r11.8/compiler.31833905_0\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Wed_Sep_21_10:33:58_PDT_2022\nCuda compilation tools, release 11.8, V11.8.89\nBuild cuda_11.8.r11.8/compiler.31833905_0\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"!pip install ViennaRNA","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ViennaRNA pandas","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tqdm\n!pip install seaborn\n!pip install plotly\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\nimport tensorflow.keras.layers as L\nfrom keras.layers import Dropout\nimport seaborn as sns\nimport tensorflow as tf\nimport plotly.express as px\nfrom sklearn.preprocessing import quantile_transform,StandardScaler,MinMaxScaler\nfrom transformers import BertConfig,TFBertModel,BertModel\nfrom keras.callbacks import EarlyStopping\nimport random\nimport math, json, os, random\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom kerastuner.tuners import Hyperband\nfrom kerastuner.tuners import RandomSearch\nfrom kerastuner import HyperParameters\nimport kerastuner as kt\nfrom sklearn import ensemble\nfrom sklearn import datasets\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.model_selection import KFold\n\nnp.random.seed(100)\nrandom.seed(100)\n\ntrain = pd.read_json('/kaggle/input/stanford-covid-vaccine/train.json', lines=True)\ntest = pd.read_json('/kaggle/input/stanford-covid-vaccine/test.json', lines=True)\nsample_df = pd.read_csv('/kaggle/input/stanford-covid-vaccine/sample_submission.csv')\n\n\n#sanity check\ntrain.head()\n\n\nAUTO = tf.data.experimental.AUTOTUNE\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n# This will tell us the columns we are predicting\npred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n\n\nsorted(train['signal_to_noise'].apply(np.round).astype(int).unique())\nnp.bincount(train['signal_to_noise'].apply(np.round).astype(int))\nsorted(train['SN_filter'].apply(np.round).astype(int).unique()) \nnp.bincount(train['SN_filter'].apply(np.round).astype(int))\ntrain = train[train['signal_to_noise'] > .25]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DATA AUGMENTATION (NCBI + OPPENVACCINE)**","metadata":{}},{"cell_type":"code","source":"# import torch\n# import numpy as np\n\n# # Assuming `generated_sequences` is loaded from 'generated_sequences2.npy' and contains softmax probabilities\n# generated_sequences = np.load('/kaggle/input/gan-sequences/gen_sequences.npy')\n\n    \n# import RNA\n# import pandas as pd\n# from tqdm import tqdm\n\n# import RNA\n# import numpy as np\n# import pandas as pd\n\n# # Load your NumPy array containing RNA sequences\n# sequences_array = np.load('/kaggle/input/gan-sequences/gen_sequences.npy')\n\n# # Define a function to predict RNA structure for a given sequence\n# def predict_rna_structure(sequence):\n#     ss, _ = RNA.fold(sequence)\n#     return ss\n\n# # Open a file to write the results incrementally\n# with open('GAN_RNAseqstr.csv', 'w') as file:\n#     file.write(\"sequence,structure\\n\")  # Write the header\n\n#     # Iterate over each sequence in the NumPy array\n#     for seq in sequences_array:\n#         # Extract the sequence as a string\n#         sequence_str = seq[0]\n        \n#         # Predict the RNA structure for the sequence\n#         structure = predict_rna_structure(sequence_str)\n        \n#         # Write the sequence and its structure to the CSV file\n#         file.write(f\"\\\"{sequence_str}\\\",\\\"{structure}\\\"\\n\")\n\n# # Read and print the dataframe\n# df = pd.read_csv('GAN_RNAseqstr.csv')\n# print(df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def read_file_in_chunks(file_path, chunk_size=1024):\n#     with open(file_path, 'r') as file:\n#         while True:\n#             chunk = file.read(chunk_size)\n#             if not chunk:\n#                 break\n#             yield chunk\n\n\n# # Specify the file paths\n# file_paths = [\n#     '/kaggle/input/sequences-ncbi-auto/downloaded_sequences_0.fasta',\n# #     '/kaggle/input/sequences-ncbi-auto/downloaded_sequences_1.fasta',\n# #     '/kaggle/input/sequences-ncbi-auto/downloaded_sequences_2.fasta',\n# #     '/kaggle/input/sequences-ncbi-auto/downloaded_sequences_3.fasta',\n# #     '/kaggle/input/sequences-ncbi-auto/downloaded_sequences_4.fasta',\n# #     '/kaggle/input/sequences-ncbi-auto/downloaded_sequences_5.fasta'\n# #     '/kaggle/input/sequences-ncbi-auto/downloaded_sequences_6.fasta'\n# ]\n\n# contents = [''.join(read_file_in_chunks(file_path)) for file_path in file_paths]#3463170954\n# contents_without_mRNA = [line for line in contents[0].split('\\n') if not line.startswith('>')]\n\n# updated_contents = ''.join(contents_without_mRNA)\n\n\n    \n# import RNA\n# import pandas as pd\n# from tqdm import tqdm\n\n# def predict_rna_structure(sequence):\n#     ss, _ = RNA.fold(sequence)\n#     return ss\n\n# def chunk_sequence(sequence, chunk_size=107):\n#     for i in range(0, len(sequence), chunk_size):\n#         yield sequence[i:i + chunk_size]\n\n# def calculate_total_chunks(sequence, chunk_size=107):\n#     return (len(sequence) + chunk_size - 1) // chunk_size\n\n# chunked_sequences = chunk_sequence(updated_contents)\n# total_chunks = calculate_total_chunks(updated_contents)\n\n# # Open a file to write the results incrementally (For NCBI GAN generated sequences)\n# with open('RNAseqstr.csv', 'w') as file:\n#     file.write(\"sequence,structure\\n\")  # Write the header\n\n#     for seq in tqdm(chunked_sequences, total=total_chunks, desc=\"Processing Sequences\"):\n#         structure = predict_rna_structure(seq)\n#         file.write(f\"\\\"{seq}\\\",\\\"{structure}\\\"\\n\")\n\n# # Read and print the dataframe\n# df = pd.read_csv('RNAseqstr.csv')\n# print(df)\n\n# # Open a file to write the results incrementally (For NCBI RNA SEQUENCES)\n# with open('RNAseqstr.csv', 'w') as file:\n#     file.write(\"sequence,structure\\n\")  # Write the header\n\n#     for seq in tqdm(chunked_sequences, total=total_chunks, desc=\"Processing Sequences\"):\n#         structure = predict_rna_structure(seq)\n#         file.write(f\"\\\"{seq}\\\",\\\"{structure}\\\"\\n\")\n\n# # Read and print the dataframe\n# df = pd.read_csv('RNAseqstr.csv')\n# print(df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NCBI_with_struc = pd.read_csv('/kaggle/input/rna-seq-str/RNAseqstr.csv')\ndef convert_t_to_u(sequence):\n    return sequence.replace('T', 'U')\n\n# Apply this function to every entry in the 'sequence' column\nNCBI_with_struc['sequence'] = NCBI_with_struc['sequence'].apply(convert_t_to_u)\n\n# file_path = '/kaggle/input/stanford-covid-vaccine/train.json'\n# data = pd.read_json(file_path, lines=True)\n# type(data)\n\n# print(type(data),data.shape,data.columns)\n\n# print(type(NCBI_with_struc),NCBI_with_struc.shape,NCBI_with_struc.columns)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n\n# # Assuming data and data2 are your DataFrames\n\n# # Merge the DataFrames\n# merged_df = pd.merge(data, NCBI_with_struc, on=['sequence', 'structure'], how='outer')\n\n# # Replace NaNs with space in the columns that are exclusive to 'data'\n# exclusive_columns = data.columns.difference(NCBI_with_struc.columns)\n# merged_df[exclusive_columns] = merged_df[exclusive_columns].fillna(' ')\n\n# # Now merged_df is your final DataFrame\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merged_df\nGAN_RNAseqstr = pd.read_csv('/kaggle/input/rna-seq-str/GAN_RNAseqstr.csv')\nprint(len(NCBI_with_struc))\n\nNCBI_with_struc = pd.merge(GAN_RNAseqstr, NCBI_with_struc, on=['sequence', 'structure'], how='outer')\nlen(NCBI_with_struc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MASKING: (structure, sequence)**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertModel, BertConfig\nfrom torch.optim.lr_scheduler import OneCycleLR\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForMaskedLM\nfrom torch.optim import AdamW\nimport numpy as np\nfrom transformers import BertConfig\nimport matplotlib.pyplot as plt\ntrain_losses = []\nval_losses = []\n\n# file_path = '/kaggle/input/stanford-covid-vaccine/train.json'\n# data = pd.read_json(file_path, lines=True)  ####merged_df\ndata = NCBI_with_struc\n# Define the masking probability\nmasking_probability = 0.25\ntoken2int = {'A': 1, 'C': 2, 'G': 3, 'U': 4, '(': 5, ')': 6, '.': 7, '[MASK]': 8}\n# Custom Dataset for RNA\nclass RNADataset(Dataset):\n    def __init__(self, dataframe):\n        self.data = dataframe\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        masked_sequence = self.data.iloc[idx]['masked_sequence']\n        masked_structure = self.data.iloc[idx]['masked_structure']\n        original_sequence = self.data.iloc[idx]['original_sequence']\n        return masked_sequence, masked_structure, original_sequence\n\ndef collate_fn(batch):\n    sequences, structures, original_sequences = zip(*batch)\n    sequence_ids = []\n    structure_ids = []\n    label_ids = []\n\n    # Determine the maximum length in this batch\n    max_length = max(max(len(seq) for seq in sequences), max(len(struct) for struct in structures))\n\n    for seq, struct, orig_seq in zip(sequences, structures, original_sequences):\n        seq_ids = [token2int.get(token, token2int['[MASK]']) for token in seq]\n        struct_ids = [token2int.get(token, token2int['[MASK]']) for token in struct]\n        orig_seq_ids = [token2int.get(token, token2int['[MASK]']) for token in orig_seq]\n\n        # Padding\n        seq_ids.extend([token2int['[MASK]']] * (max_length - len(seq_ids)))\n        struct_ids.extend([token2int['[MASK]']] * (max_length - len(struct_ids)))\n        orig_seq_ids.extend([token2int['[MASK]']] * (max_length - len(orig_seq_ids)))\n\n        sequence_ids.append(seq_ids)\n        structure_ids.append(struct_ids)\n        label_ids.append(orig_seq_ids)\n\n    sequence_ids = torch.tensor(sequence_ids)\n    structure_ids = torch.tensor(structure_ids)\n    label_ids = torch.tensor(label_ids)\n\n    input_ids = torch.stack([sequence_ids, structure_ids], dim=1)\n    attention_masks = torch.ones_like(sequence_ids)\n\n    return {'input_ids': input_ids, 'attention_mask': attention_masks, 'labels': label_ids}\n\ndef mask_dynamic(entity, dynamic_type):\n    if dynamic_type == 'contiguous':\n        start = np.random.randint(0, len(entity)-1)\n        end = np.random.randint(start, len(entity))\n        entity = entity[:start] + '[MASK]' * (end - start + 1) + entity[end + 1:]\n    elif dynamic_type == 'gapped':\n        gaps = np.random.choice([True, False], len(entity), p=[0.2, 0.8])\n        entity = ''.join([e if not g else '[MASK]' for e, g in zip(entity, gaps)])\n    return entity\n\ndef mask_sequence(sequence):\n    mask = np.random.rand(len(sequence)) < masking_probability\n    masked_positions = [i for i, m in enumerate(mask) if m]  # List to store the positions where masking occurs\n    masked_sequence = ''.join(['[MASK]' if m else seq for seq, m in zip(sequence, mask)])\n    return masked_sequence, masked_positions\n\ndef mask_structure(structure, masked_positions,sequence):\n    \n    masked_structure = ''\n    for i, char in enumerate(structure):\n        if i in masked_positions:\n            masked_structure += '[MASK]'  # Apply masking at the same positions as in the sequence\n        if sequence[i] == 'G' and np.random.rand() < 0.3:  # 30% chance to mask if nucleotide is G\n            masked_structure += '[MASK]'  # Apply masking at positions where sequence has 'G'\n        else:\n            masked_structure += char\n\n    return masked_structure\n\n\n# Assuming 'data' is your DataFrame containing the RNA sequences and structures\nfor idx, row in NCBI_with_struc.iterrows():\n    masked_sequence, masked_positions = mask_sequence(row['sequence'])\n    masked_structure = mask_structure(row['structure'], masked_positions,row['sequence'])\n    NCBI_with_struc.at[idx, 'masked_sequence'] = masked_sequence\n    NCBI_with_struc.at[idx, 'masked_structure'] = masked_structure\n\nprint(NCBI_with_struc[['masked_sequence', 'masked_structure']])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(NCBI_with_struc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass CustomBERTModel(nn.Module):\n    def __init__(self, config):\n        super(CustomBERTModel, self).__init__()\n        self.sequence_embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n        self.structure_embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n\n        # Flattening layer: adjust the input size as needed\n        self.flatten = nn.Linear(2 * config.hidden_size, config.hidden_size)\n\n        # BertModel contains the transformer layers\n        self.bert = BertModel(config)\n        self.classifier = nn.Linear(config.hidden_size, config.vocab_size)  # Assuming the output size is vocab_size\n\n    def forward(self, input_ids, attention_mask):\n        # Check the shape of input_ids to confirm it's a 3D tensor\n        if input_ids.dim() != 3 or input_ids.size(1) != 2:\n            raise ValueError(f\"Expected input_ids of shape [batch_size, 2, sequence_length], got {input_ids.shape}\")\n\n        seq_input_ids = input_ids[:, 0, :]  # Sequence component\n        struct_input_ids = input_ids[:, 1, :]  # Structure component\n\n\n        # Get embeddings\n        seq_embeddings = self.sequence_embedding(seq_input_ids)\n        struct_embeddings = self.structure_embedding(struct_input_ids)\n\n        # Concatenate and flatten embeddings\n        combined_embeddings = torch.cat((seq_embeddings, struct_embeddings), dim=2)\n        flat_embeddings = self.flatten(combined_embeddings)\n\n        # Get the last hidden state\n        bert_output = self.bert(inputs_embeds=flat_embeddings, attention_mask=attention_mask)\n        last_hidden_state = bert_output.last_hidden_state\n\n        # Predictions\n        predictions = self.classifier(last_hidden_state)\n\n        return predictions\n# Initialize the Dataset and DataLoader\n\nNCBI_with_struc['original_sequence'] = NCBI_with_struc['sequence']\ndataset = RNADataset(NCBI_with_struc)\nprint(type(dataset), dataset)\n\n\nfrom sklearn.model_selection import train_test_split\n\n# Assuming 'data' is your DataFrame containing the RNA sequences and structures\n# Split data into training and validation sets\ntrain_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n\n# Create training and validation datasets\ntrain_dataset = RNADataset(train_data)\nval_dataset = RNADataset(val_data)\n\n# Create DataLoaders for training and validation datasets\ntrain_loader = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn)\n\n# data_loader = DataLoader(dataset, batch_size=8, collate_fn=collate_fn)\n\n# Config for BERT\nbert_config = BertConfig(\n    vocab_size=800,  # Example vocab size\n    hidden_size=128,\n    num_hidden_layers=8,\n    num_attention_heads=8,\n    intermediate_size=500\n)\n\n# Initialize the model\nmodel = CustomBERTModel(bert_config)\n\n# Example input (batch_size=2, sequence_length=107)\ninput_ids = torch.randint(0, 800, (2, 2, 107))\nattention_mask = torch.ones(2, 107)\n\n# Forward pass\noutputs = model(input_ids, attention_mask)\n\n\n# Initialize BERT model from scratch\noptimizer = AdamW(model.parameters(), lr=1e-5)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Training Loop\nmodel.train()\nnum_epochs = 2\n# Assuming you are using CrossEntropyLoss for a masked language modeling task\nloss_function = nn.CrossEntropyLoss()\n\n# Learning rate scheduler setup\nscheduler = OneCycleLR(optimizer, max_lr=1e-4, steps_per_epoch=len(train_loader), epochs=num_epochs)\n\n# Early stopping parameters\nearly_stopping_patience = 5\nbest_val_loss = float('inf')\nepochs_without_improvement = 0\n\nfor epoch in range(num_epochs):\n    epoch_loss = 0.0\n    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n\n    for batch in progress_bar:\n        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n        labels = batch['labels'].to(device)\n\n        # Forward pass\n        predictions = model(**inputs)  # Directly use the model's output as predictions\n\n        # Reshape labels to match the output format and compute loss\n        labels = labels.view(-1)  # Flatten labels if necessary\n        predictions = predictions.view(-1, predictions.size(-1))  # Reshape for CrossEntropyLoss\n\n        loss = loss_function(predictions, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        scheduler.step()  # Update the learning rate\n\n        # Accumulate loss\n        epoch_loss += loss.item()\n        progress_bar.set_postfix({'loss': epoch_loss / (progress_bar.last_print_n + 1)})\n\n    train_losses.append(epoch_loss / len(train_loader))\n\n    print(f'Epoch: {epoch + 1}, Loss: {epoch_loss / len(train_loader)}')\n\n\n\n    # Validation phase\n    val_loss = 0.0\n    model.eval()\n    with torch.no_grad():\n        for batch in val_loader:  # Assuming you have a validation dataloader\n            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n            labels = batch['labels'].to(device)\n\n            predictions = model(**inputs)\n            labels = labels.view(-1)\n            predictions = predictions.view(-1, predictions.size(-1))\n            loss = loss_function(predictions, labels)\n            val_loss += loss.item()\n\n    val_loss /= len(val_loader)\n    val_losses.append(val_loss)\n\n    print(f'Validation Loss: {val_loss}')\n\n    # Early stopping check\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        epochs_without_improvement = 0\n        torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pt')  # Save best model\n    else:\n        epochs_without_improvement += 1\n        if epochs_without_improvement >= early_stopping_patience:\n            print(\"Early stopping triggered\")\n            break\n\n        \nmodel_save_path = 'bert_for_masked_lm_rna.pth'\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")\nplt.figure(figsize=(10, 5))\nplt.title(\"Training and Validation Loss\")\nplt.plot(train_losses, label=\"Training loss\")\nplt.plot(val_losses, label=\"Validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntoken2int = {'A': 1, 'C': 2, 'G': 3, 'U': 4, '(': 5, ')': 6, '.': 7, 'B': 8, 'E': 9, 'H': 10, 'I': 11, 'M': 12, 'S': 13, 'X': 14, '[MASK]': 15}\n\n# def preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\ndef preprocess_inputs(df, cols=['sequence']):\n\n    base_fea = np.transpose(np.array(df[cols].applymap(lambda seq: [token2int[x] for x in seq]).values.tolist()),(0, 2, 1))\n    print(base_fea.shape)\n    return np.transpose(np.concatenate([base_fea], 2),(0, 2, 1))\n\ntrain_inputs = preprocess_inputs(train)\ntrain_labels = np.array(train[pred_cols].values.tolist()).transpose((0, 2, 1))\n# train_inputs = torch.tensor(train_inputs, dtype=torch.float32)\n# train_labels = torch.tensor(train_labels, dtype=torch.float32)\n\n# trainLabels = train[pred_cols]\n# print(\"trainLabels.shape\",trainLabels.shape)\n\n# public_df = test.query(\"seq_length == 107\").copy()\n# private_df = test.query(\"seq_length == 130\").copy()\n\n# public_inputs = preprocess_inputs(public_df)\n# private_inputs = preprocess_inputs(private_df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(train_inputs[0].shape)\n# print(train_labels[0].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(np.array_equal(train_inputs[0], train_inputs1[0][4]))\n# print(train_labels1[0] == train_labels[0])  \n# print(train_inputs1[0][0].shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport time\nfrom sklearn.model_selection import KFold\nfrom transformers import BertConfig, BertModel, BertTokenizer\nimport matplotlib.pyplot as plt\nfrom transformers import BertModel, BertTokenizer\n\n\n# Define constants\nkf = KFold(n_splits=4, shuffle=True, random_state=42)\npred_len = 68\nMAX_EPOCHS = 350\nSeq_Len = 107\nbatch_size = 64\ntrain_losses = []\nval_losses = []\n\n# Initialize lists for storing fold-wise average losses\nfold_train_losses = []\nfold_val_losses = []\n\n# Initialize lists to store training histories\nhistories = []\n\n\n# Define custom loss function\ndef mcrmse(y_true, y_pred):\n    colwise_mse = torch.mean((y_true - y_pred) ** 2, dim=1)\n    mcrmse = torch.mean(torch.sqrt(colwise_mse))\n    return mcrmse\n\nclass RNADegradationModel(nn.Module):\n    def __init__(self, custom_bert_model, seq_len=107, pred_len=68, hidden_dim=64):\n        super(RNADegradationModel, self).__init__()\n        self.bert = custom_bert_model.bert\n        bert_hidden_size = self.bert.config.hidden_size  # Access hidden size from the BERT model\n   \n        self.truncate = nn.Linear(seq_len * bert_hidden_size, pred_len * hidden_dim)\n        self.output = nn.Linear(hidden_dim, 5)  # Output 5 values per position\n        self.hidden_dim = hidden_dim  # Ensure this is correctly initialized\n       \n    def forward(self, input_ids, attention_mask=None):\n        bert_embeddings = self.bert(input_ids, attention_mask=attention_mask)[0]\n        batch_size = bert_embeddings.size(0)\n        flattened_embeddings = bert_embeddings.view(batch_size, -1)\n\n        # Ensure the size matches with self.truncate input size\n        truncated = self.truncate(flattened_embeddings)  # Adjusted to the correct shape\n        truncated = truncated.view(-1, pred_len, self.hidden_dim)\n\n        all_outputs = []\n        for i in range(pred_len):\n            prediction = truncated[:, i, :]\n            output_i = self.output(prediction)\n            all_outputs.append(output_i.unsqueeze(1))\n\n        predicted_output = torch.cat(all_outputs, dim=1)\n        return predicted_output\n\n\ncustom_bert_model = CustomBERTModel(bert_config)\ncustom_bert_model.load_state_dict(torch.load('bert_for_masked_lm_rna.pth'))\n\ntrain_losses_per_fold = []  # List to store losses for all folds\n\nfor fold, (idxT, idxV) in enumerate(kf.split(train_inputs)):\n    fold_specific_train_losses = []\n    fold_specific_val_losses = []\n\n    custom_model = RNADegradationModel(custom_bert_model, seq_len=107, pred_len=68, hidden_dim=bert_config.hidden_size)\n    custom_model.to(device)\n\n    # Convert NumPy arrays to PyTorch tensors\n    trn_ = torch.tensor(train_inputs[idxT, :, :], dtype=torch.float32).to(device)\n    trn_labs = torch.tensor(train_labels[idxT, :, :], dtype=torch.float32).to(device)\n\n    val_ = torch.tensor(train_inputs[idxV, :, :], dtype=torch.float32).to(device)\n    val_labs = torch.tensor(train_labels[idxV, :, :], dtype=torch.float32).to(device)\n\n\n\n    optimizer = optim.AdamW(custom_model.parameters(), lr=1e-4)\n#     optimizer = optim.SGD(custom_model.parameters(), lr=1e-4)\n    train_inputs = torch.tensor(train_inputs, dtype=torch.float32).to(device)\n    train_labels = torch.tensor(train_labels, dtype=torch.float32).to(device)\n\n    accumulation_steps = 4  # Adjust as needed\n    SAVE_INTERVAL = 5  # Save the model every 5 epochs\n\n    for epoch in range(MAX_EPOCHS):\n        custom_model.train()\n        total_loss = 0\n        num_batches = int(np.ceil(len(train_inputs) / batch_size))\n\n        for i in range(num_batches):\n            optimizer.zero_grad()  # Clear gradients at the start of each batch\n\n            for j in range(accumulation_steps):\n                # Calculate start and end indices for the current accumulated batch\n                start_index = i * batch_size + j * (batch_size // accumulation_steps)\n                end_index = start_index + (batch_size // accumulation_steps)\n\n                # Ensure not to exceed the array bounds\n                end_index = min(end_index, len(train_inputs))\n                if end_index <= start_index:\n                    continue  # Skip this iteration if the batch is empty\n\n                # Fetch the sub-batch data\n                inputs = train_inputs[start_index:end_index].squeeze(1)\n                inputs = inputs.long().to(device)\n                labels = train_labels[start_index:end_index]\n                labels = labels.to(device)\n\n                # Forward pass and loss computation\n                outputs = custom_model(inputs)\n                loss = mcrmse(labels, outputs)\n                loss = loss / accumulation_steps  # Normalize the loss (important when accumulating)\n\n                # Backward pass (accumulates gradients)\n                loss.backward()\n                total_loss += loss.item()\n\n            # Perform the optimization step after accumulating gradients\n            optimizer.step()\n\n        avg_train_loss = total_loss / num_batches\n        train_losses.append(avg_train_loss)\n\n    #     torch.save(custom_model.state_dict(), f'model_epoch_{epoch+1}.pt')\n        print(f\"Epoch {epoch+1}/{MAX_EPOCHS}, Train Loss: {total_loss / num_batches}\")\n\n        num_val_batches = int(np.ceil(len(val_) / batch_size))\n\n        # Validation\n        custom_model.eval()\n        total_val_loss = 0  # Initialize total validation loss\n        with torch.no_grad():\n            for i in range(num_val_batches):\n                start_index = i * batch_size\n                end_index = start_index + batch_size\n\n                val_inputs = val_[start_index:end_index]\n                val_labels = val_labs[start_index:end_index]\n\n                # Ensure correct dimensions for val_inputs\n                if val_inputs.dim() > 2:\n                    val_inputs = torch.squeeze(val_inputs, dim=1)\n                if val_inputs.dim() > 2:\n                    val_inputs = val_inputs.view(-1, val_inputs.size(-1))\n                val_inputs = val_inputs.long()\n\n                val_outputs = custom_model(val_inputs)\n                val_batch_loss = mcrmse(val_labels, val_outputs)\n                val_loss = torch.mean(val_batch_loss)\n                total_val_loss += val_loss.item()  # Accumulate the validation loss\n\n        avg_val_loss = total_val_loss / num_val_batches  # Calculate the average validation loss\n        val_losses.append(avg_val_loss)\n        fold_specific_train_losses.append(avg_train_loss)\n        fold_specific_val_losses.append(avg_val_loss)\n\n        # Store the average validation loss for this epoch\n        histories.append({'loss': total_loss / num_batches, 'val_loss': avg_val_loss})\n\n        # Save the model's state_dict to a file\n\n        if (epoch + 1) % 10 == 0 or (epoch + 1) == MAX_EPOCHS:\n            torch.save(custom_model.state_dict(), f'model_epoch_{epoch+1}.pt')\n            print(f\"Saved model at fold {fold}, epoch {epoch + 1}\")\n\n            \n    # Store losses\n    fold_train_losses.append(fold_specific_train_losses)\n    fold_val_losses.append(fold_specific_val_losses)\n\n\n    # Plot training histories for each fold\n    for fold, history in enumerate(histories):\n        print(f\"Fold {fold+1} - Training Loss:\", history['loss'])\n        print(f\"Fold {fold+1} - Validation Loss:\", history['val_loss'])\n\n\n# # If you want to plot these as well, you can create lists and plot them\n# fold_train_losses = [history['loss'] for history in histories]\n# fold_val_losses = [history['val_loss'] for history in histories]\n\n\n# Calculate average losses across folds\naverage_train_losses = np.mean(fold_train_losses, axis=0)\naverage_val_losses = np.mean(fold_val_losses, axis=0)\n\n# Plot the average loss\nplt.figure(figsize=(10, 5))\nplt.title(\"Training and Validation Loss\")\nplt.plot(average_train_losses, label=\"Average Training loss\")  # Plot average training loss\nplt.plot(average_val_losses, label=\"Average Validation loss\")  # Plot average validation loss\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inspect the first batch\nfirst_batch = next(iter(train_loader))\nprint(type(first_batch))\nprint(len(first_batch))\n\n# If it's a tuple or list, you can check the contents\nif isinstance(first_batch, (tuple, list)):\n    print([type(item) for item in first_batch])\n    print([item.shape if isinstance(item, torch.Tensor) else None for item in first_batch])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import tensorflow as tf\n# import time\n# from sklearn.model_selection import KFold\n# from transformers import BertConfig, TFBertModel\n# from tensorflow.keras.layers import Input, Flatten, Dense\n# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n# import matplotlib.pyplot as plt\n\n# # Define constants\n# kf = KFold(n_splits=4, shuffle=True, random_state=42)\n# SEED = 1\n# Seq_Len = 119\n# Pred_Len = 68\n# HYPERBAND_MAX_EPOCHS = 10\n# MAX_TRIALS = 20\n# EXECUTION_PER_TRIAL = 10\n# N_EPOCH_SEARCH = 20\n\n# Seq_Len = 107\n\n# Define custom loss function\nimport torch\n\ndef mcrmse(y_true, y_pred):\n    colwise_mse = torch.mean((y_true - y_pred) ** 2, dim=1)\n    return torch.mean(torch.sqrt(colwise_mse), dim=1)\n\n# # Initialize early stopping\n# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n\n# # Function to build your model\n# from transformers import BertTokenizer, TFBertModel\n# import tensorflow as tf\n\n# # Load the tokenizer associated with the pretrained model\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# # Create an input layer for your sequences\n# input_layer = tf.keras.layers.Input(shape=(Seq_Len,), dtype=tf.int32)\n\n# # Load the pre-trained BERT model\n# base_model = TFBertModel.from_pretrained('/kaggle/working/bert_for_masked_lm_rna')\n\n# # Extract BERT embeddings for your input sequences\n# bert_embeddings = base_model(input_layer)[0]\n\n# # You can continue building your custom layers and outputs on top of the BERT embeddings\n# # Example: Add a Dense layer for regression\n# dense_layer = tf.keras.layers.Dense(units=5, activation='linear')(bert_embeddings)\n\n# # Create the final model\n# model = tf.keras.models.Model(inputs=input_layer, outputs=dense_layer)\n\n# # Modify the build_model function to use your pretrained BERT model\n# def build_model(seq_len=107, pred_len=68, dropout=0.5, embed_dim=100, hidden_dim=128):\n#     ids = Input(shape=(5, seq_len,), dtype=tf.int32)\n\n#     # Use your pretrained BERT model as the base model\n#     bert_embeddings = base_model(ids)[0]\n\n#     truncated = bert_embeddings[:, :pred_len, :]\n#     out = Dense(5, activation='linear')(truncated)\n#     model = tf.keras.Model(inputs=ids, outputs=out)\n\n#     return model\n\n# # The rest of your code remains the same\n\n\n# # Record the start time\n# start_time = time.time()\n\n# # Initialize lists to store training histories\n# histories = []\n\n# # Loop through K-Folds\n# for fold, (idxT, idxV) in enumerate(kf.split(train_inputs)):\n#     trn_ = train_inputs[idxT, :, :]\n#     trn_labs = train_labels[idxT, :, :]\n\n#     val_ = train_inputs[idxV, :, :]\n#     val_labs = train_labels[idxV, :, :]\n\n#     # Build and train the model\n#     model = build_model(seq_len=Seq_Len, pred_len=Pred_Len)\n#     history = model.fit(\n#         trn_, trn_labs,\n#         validation_data=(val_, val_labs),\n#         batch_size=64,\n#         epochs=50,\n#         callbacks=[\n#             tf.keras.callbacks.ReduceLROnPlateau(),\n#             ModelCheckpoint('model' + str(fold) + '.h5', save_weights_only=True, save_best_only=True)\n#         ]\n#     )\n#     histories.append(history)\n\n#     # Load model weights\n#     model_short = build_model(seq_len=107, pred_len=107)\n#     model_long = build_model(seq_len=130, pred_len=130)\n\n#     model_short.load_weights('model' + str(fold) + '.h5')\n#     model_long.load_weights('model' + str(fold) + '.h5')\n\n#     if fold == 0:\n#         public_preds = model_short.predict([public_inputs]) / 5\n#         private_preds = model_long.predict([private_inputs]) / 5\n#     else:\n#         public_preds += model_short.predict([public_inputs]) / 5\n#         private_preds += model_long.predict([private_inputs]) / 5\n\n#     # Compute and print validation metrics\n#     val_losses = []\n#     y_preds_best = None\n#     PRED_COLS = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C', 'deg_pH10', 'deg_50C']\n#     y_valid = val_labs\n#     y_preds = model.predict(val_)\n\n#     mcloss = mcrmse(val_labs, y_preds)\n#     val_losses.append(mcloss)\n#     print('np.min(val_losses)', np.min(val_losses), 'val_losses', np.mean(val_losses), 'mcloss', np.mean(mcloss))\n\n#     if np.min(np.mean(val_losses)) == np.mean(mcloss):\n#         y_preds_best = y_preds\n\n#     print('y_preds_best shape, y_valid shape', y_valid.shape, y_preds_best.shape)\n#     fig, ax = plt.subplots(1, 3, figsize=(24, 8))\n#     for i, p in enumerate(PRED_COLS):\n#         ax[2].scatter(y_valid[:, :, i].flatten(), y_preds_best[:, :, i].flatten(), alpha=0.5)\n\n#     ax[2].legend(PRED_COLS)\n#     ax[2].set_xlabel('y_true')\n#     ax[2].set_ylabel('y_predicted')\n#     plt.show()\n\n# # Record the end time\n# end_time = time.time()\n\n# # Calculate the duration\n# duration = end_time - start_time\n# print(\"Duration:\", duration)\n\n# # Plot training histories\n# results = {\"models\": ['bert', 'bert'], \"histories\": [histories, histories]}\n# fig, ax = plt.subplots(1, len(results['histories']), figsize=(20, 10))\n# fig2, ax2 = plt.subplots(1, len(results['histories']), figsize=(20, 10))\n\n# for i, result in enumerate(results['histories']):\n#     accum_loss, accum_val_loss, accum_mae, accum_mse = 0, 0, 0, 0\n#     accum_tf_pearson, accum_val_tf_pearson = 0, 0\n\n#     n = len(result)\n#     for history in result:\n#         accum_loss += np.array(history.history['loss'])\n#         accum_val_loss += np.array(history.history['val_loss'])\n#         accum_mae += np.array(history.history['mae'])\n#         accum_mse += np.array(history.history['mse'])\n#         accum_tf_pearson += np.array(history.history['tf_pearson'])\n#         accum_val_tf_pearson += np.array(history.history['val_tf_pearson'])\n\n#     avg_loss = accum_loss / n\n#     avg_val_loss = accum_val_loss / n\n#     avg_mae = accum_mae / n\n#     avg_mse = accum_mse / n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}